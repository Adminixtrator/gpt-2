{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2_With_SQuAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adminixtrator/gpt-2/blob/master/GPT_2_With_SQuAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1z7jCKQ2Hri",
        "colab_type": "text"
      },
      "source": [
        "# Calling file from Repository\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMrNoXGt2OSp",
        "colab_type": "code",
        "outputId": "00f9332f-856f-429a-8e6d-383e09e594d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!git clone https://github.com/adminixtrator/gpt-2.git\n",
        "\n",
        "%cd gpt-2\n",
        "%ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 93 (delta 27), reused 58 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (93/93), done.\n",
            "/content/gpt-2\n",
            "Dockerfile.cpu     evaluate-v1.1.py  LICENSE           run_squad.py\n",
            "Dockerfile.gpu     evaluate-v2.0.py  model_card.md     setup.py\n",
            "domains.txt        GPT_2.ipynb       README.md         \u001b[0m\u001b[01;34mSQuAD\u001b[0m/\n",
            "download_model.py  gpt2_squad.py     requirements.txt  \u001b[01;34msrc\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4KGursR0rdo",
        "colab_type": "text"
      },
      "source": [
        "# Using the gpt-2 model 345M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgycuUVz0llH",
        "colab_type": "code",
        "outputId": "0ceb960c-5e76-4854-af48-00b6e72282cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#Download the gpt-2 model 345M..\n",
        "!python3 download_model.py 345M"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 740kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 55.2Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 540kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:20, 69.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.17Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 50.3Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 53.0Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMeHjf3X1x_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encoding..\n",
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Une1DBFK6k1-",
        "colab_type": "text"
      },
      "source": [
        "# Now to Implementing gpt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51fzEHKv5sl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Changing directory..\n",
        "import os\n",
        "\n",
        "os.chdir('src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN_q2R26OnPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "cf5ec930-7985-4341-fe5e-6c7db33ac5a6"
      },
      "source": [
        "!pip install regex   #For OpenAI GPT"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609235 sha256=24902dd99f62e2432650fc8bc03a210d6ad8589f0287421ffb1c8c6de1a968a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2019.8.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQuw66yc668Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the necessary libraries..\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkuOzFcyCqr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to use the interaction model..\n",
        "\n",
        "def interact_model(model_name, seed, nsamples, batch_size, length, temperature, top_k, models_dir):\n",
        "  \n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(hparams=hparams, length=length, context=context, batch_size=batch_size, temperature=temperature, top_k=top_k)\n",
        "\n",
        "        saver = tf.train.Saver(save_relative_paths=True)\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"\\nModel prompt >>> \")\n",
        "            if raw_text == 'ADMIN_NIXTRATOR':\n",
        "              raw_text = False\n",
        "              break\n",
        "            while not raw_text:\n",
        "                print('\\nPrompt should not be empty!')\n",
        "                raw_text = input(\"\\nModel prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)\n",
        "            \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WekJLJV_HEs9",
        "colab_type": "text"
      },
      "source": [
        "# **Code Explanation**\n",
        "\n",
        "## **model_name**: \n",
        " This indicates which model we are using. In our case, we are using the GPT-2 model with 345 million parameters or weights\n",
        "\n",
        "## **seed**: \n",
        "Integer seed for random number generators, fix seed to reproduce results\n",
        "\n",
        "## **nsamples**: \n",
        "This represents the number of sample texts generated in our output\n",
        "\n",
        "## **batch_size**: \n",
        "This only affects speed/memory. This must also divide nsamples\n",
        "\n",
        "*Note: To generate more than one sample, you need to change the values of both nsamples and batch_size and also have to keep them equal.*\n",
        "\n",
        "## **length**: \n",
        "It represents the number of tokens in the generated text. If the length is None, then the number of tokens is decided by model hyperparameters\n",
        "\n",
        "## **temperature**: \n",
        "This controls randomness in Boltzmann distribution. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions\n",
        "\n",
        "## **top_k**:  \n",
        "This parameter controls diversity. If the value of top_k is set to 1, this means that only 1 word is considered for each step (token). If top_k is set to 40, that means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. top_k = 40 generally is a good value\n",
        "\n",
        "## **models_dir**: \n",
        "It represents the path to parent folder containing model subfolders (contains the <model_name> folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPLy_MvMIjJy",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ikkNd_Gl4t",
        "colab_type": "code",
        "outputId": "b6f12965-a34c-411a-9bb0-0796cf4a2a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "#Using the arguements above..\n",
        "\n",
        "interact_model('345M', None, 1, 1, 20, 1, 0, '/content/gpt-2/models')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "INFO:tensorflow:Restoring parameters from /content/gpt-2/models/345M/model.ckpt\n",
            "\n",
            "Model prompt >>> Today we submit and\n",
            "======================================== SAMPLE 1 ========================================\n",
            " agree to the following terms of use:\n",
            "\n",
            "The software protocols are verifiable on the openS\n",
            "================================================================================\n",
            "\n",
            "Model prompt >>> Today we move to stage 6\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", how small an amount of further trauma has befallen flying, how much less we should have\n",
            "================================================================================\n",
            "\n",
            "Model prompt >>> Today,\n",
            "======================================== SAMPLE 1 ========================================\n",
            " I've spoken to Senna many times in a variety of ways; from enthusing in conversation\n",
            "================================================================================\n",
            "\n",
            "Model prompt >>> ADMIN_NIXTRATOR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2mX9jZR4umF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFVgnfhgfcZw",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning on SQuAD for question-answering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBsFk4jWgA_P",
        "colab_type": "code",
        "outputId": "5af50b3a-1c0f-443e-9c07-64050076f313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Checking Directory..\n",
        "os.chdir('/content/gpt-2/SQuAD/')\n",
        "%ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "create_emb.ipynb  \u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mInferSent\u001b[0m/  models.py  unsupervised.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJxsBWzUy2VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the neccessary libraries..\n",
        "import numpy as np, pandas as pd\n",
        "import json\n",
        "import ast \n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "import torch\n",
        "import pickle\n",
        "from scipy import spatial\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import spacy\n",
        "from nltk import Tree\n",
        "en_nlp = spacy.load('en')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "st = LancasterStemmer()\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8omyb-qDwNmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train set \n",
        "train = pd.read_json(\"data/train-v2.0.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy5AsaWCJIoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "094429d1-a9fa-494e-f0c7-e643e9c3d211"
      },
      "source": [
        "#Familiarizing with the dataset..\n",
        "train.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afIbljbwJVqD",
        "colab_type": "text"
      },
      "source": [
        "## Loading Embedding dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOS5tvt0JPVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_target(x):\n",
        "    idx = -1\n",
        "    for i in range(len(x[\"sentences\"])):\n",
        "        if x[\"text\"] in x[\"sentences\"][i]: idx = i\n",
        "    return idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik06hJ4qJcSM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d78b04be-7c68-4882-f765-64f6ac84bab9"
      },
      "source": [
        "train.data"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1      {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2      {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3      {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4      {'title': 'The_Legend_of_Zelda:_Twilight_Princ...\n",
              "5      {'title': 'Spectre_(2015_film)', 'paragraphs':...\n",
              "6      {'title': '2008_Sichuan_earthquake', 'paragrap...\n",
              "7      {'title': 'New_York_City', 'paragraphs': [{'qa...\n",
              "8      {'title': 'To_Kill_a_Mockingbird', 'paragraphs...\n",
              "9      {'title': 'Solar_energy', 'paragraphs': [{'qas...\n",
              "10     {'title': 'Kanye_West', 'paragraphs': [{'qas':...\n",
              "11     {'title': 'Buddhism', 'paragraphs': [{'qas': [...\n",
              "12     {'title': 'American_Idol', 'paragraphs': [{'qa...\n",
              "13     {'title': 'Dog', 'paragraphs': [{'qas': [{'que...\n",
              "14     {'title': '2008_Summer_Olympics_torch_relay', ...\n",
              "15     {'title': 'Genome', 'paragraphs': [{'qas': [{'...\n",
              "16     {'title': 'Comprehensive_school', 'paragraphs'...\n",
              "17     {'title': 'Republic_of_the_Congo', 'paragraphs...\n",
              "18     {'title': 'Prime_minister', 'paragraphs': [{'q...\n",
              "19     {'title': 'Institute_of_technology', 'paragrap...\n",
              "20     {'title': 'Wayback_Machine', 'paragraphs': [{'...\n",
              "21     {'title': 'Dutch_Republic', 'paragraphs': [{'q...\n",
              "22     {'title': 'Symbiosis', 'paragraphs': [{'qas': ...\n",
              "23     {'title': 'Canadian_Armed_Forces', 'paragraphs...\n",
              "24     {'title': 'Cardinal_(Catholicism)', 'paragraph...\n",
              "25     {'title': 'Iranian_languages', 'paragraphs': [...\n",
              "26     {'title': 'Lighting', 'paragraphs': [{'qas': [...\n",
              "27     {'title': 'Separation_of_powers_under_the_Unit...\n",
              "28     {'title': 'Architecture', 'paragraphs': [{'qas...\n",
              "29     {'title': 'Human_Development_Index', 'paragrap...\n",
              "                             ...                        \n",
              "412    {'title': 'University', 'paragraphs': [{'qas':...\n",
              "413    {'title': 'Religion_in_ancient_Rome', 'paragra...\n",
              "414    {'title': 'YouTube', 'paragraphs': [{'qas': [{...\n",
              "415    {'title': 'Separation_of_church_and_state_in_t...\n",
              "416    {'title': 'Protestantism', 'paragraphs': [{'qa...\n",
              "417    {'title': 'Bras%C3%ADlia', 'paragraphs': [{'qa...\n",
              "418    {'title': 'Economy_of_Greece', 'paragraphs': [...\n",
              "419    {'title': 'Party_leaders_of_the_United_States_...\n",
              "420    {'title': 'Armenians', 'paragraphs': [{'qas': ...\n",
              "421    {'title': 'Jehovah%27s_Witnesses', 'paragraphs...\n",
              "422    {'title': 'Dwight_D._Eisenhower', 'paragraphs'...\n",
              "423    {'title': 'The_Bronx', 'paragraphs': [{'qas': ...\n",
              "424    {'title': 'Financial_crisis_of_2007%E2%80%9308...\n",
              "425    {'title': 'Portugal', 'paragraphs': [{'qas': [...\n",
              "426    {'title': 'Humanism', 'paragraphs': [{'qas': [...\n",
              "427    {'title': 'Geological_history_of_Earth', 'para...\n",
              "428    {'title': 'Police', 'paragraphs': [{'qas': [{'...\n",
              "429    {'title': 'Genocide', 'paragraphs': [{'qas': [...\n",
              "430    {'title': 'Saint_Barth%C3%A9lemy', 'paragraphs...\n",
              "431    {'title': 'Tajikistan', 'paragraphs': [{'qas':...\n",
              "432    {'title': 'University_of_Notre_Dame', 'paragra...\n",
              "433    {'title': 'Anthropology', 'paragraphs': [{'qas...\n",
              "434    {'title': 'Montana', 'paragraphs': [{'qas': [{...\n",
              "435    {'title': 'Punjab,_Pakistan', 'paragraphs': [{...\n",
              "436    {'title': 'Richmond,_Virginia', 'paragraphs': ...\n",
              "437    {'title': 'Infection', 'paragraphs': [{'qas': ...\n",
              "438    {'title': 'Hunting', 'paragraphs': [{'qas': [{...\n",
              "439    {'title': 'Kathmandu', 'paragraphs': [{'qas': ...\n",
              "440    {'title': 'Myocardial_infarction', 'paragraphs...\n",
              "441    {'title': 'Matter', 'paragraphs': [{'qas': [{'...\n",
              "Name: data, Length: 442, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICyiNTqmJu0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsYz0cqaKrL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68730258-9be5-4210-c6d8-d295da158aee"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRKi6S_LQVkh",
        "colab_type": "text"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M769nEwpKtkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(train):\n",
        "    \n",
        "    print(\"step 1\")\n",
        "    train['sentences'] = train['context'].apply(lambda x: [item.raw for item in TextBlob(x).sentences])\n",
        "    \n",
        "    print(\"step 2\")\n",
        "    train[\"target\"] = train.apply(get_target, axis = 1)\n",
        "    \n",
        "    print(\"step 3\")\n",
        "    train['sent_emb'] = train['sentences'].apply(lambda x: [dict_emb[item][0] if item in\\\n",
        "                                                           dict_emb else np.zeros(4096) for item in x])\n",
        "    print(\"step 4\")\n",
        "    train['quest_emb'] = train['question'].apply(lambda x: dict_emb[x] if x in dict_emb else np.zeros(4096) )\n",
        "        \n",
        "    return train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysNCcnXCK2oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train = process_data(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r59XUSI9MXyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def cosine_sim(x):\n",
        "    li = []\n",
        "    for item in x[\"sent_emb\"]:\n",
        "        li.append(spatial.distance.cosine(item,x[\"quest_emb\"][0]))\n",
        "    return li"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XIe6irSP675",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def pred_idx(distances):\n",
        "    return np.argmin(distances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vX63YiLQFYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to make predictions..\n",
        "def predictions(train):\n",
        "    \n",
        "    train[\"cosine_sim\"] = train.apply(cosine_sim, axis = 1)\n",
        "    train[\"diff\"] = (train[\"quest_emb\"] - train[\"sent_emb\"])**2\n",
        "    train[\"euclidean_dis\"] = train[\"diff\"].apply(lambda x: list(np.sum(x, axis = 1)))\n",
        "    del train[\"diff\"]\n",
        "    \n",
        "    print(\"cosine start\")\n",
        "    \n",
        "    train[\"pred_idx_cos\"] = train[\"cosine_sim\"].apply(lambda x: pred_idx(x))\n",
        "    train[\"pred_idx_euc\"] = train[\"euclidean_dis\"].apply(lambda x: pred_idx(x))\n",
        "    \n",
        "    return train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-StiXqwQRfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making predictions..\n",
        "predicted = predictions(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_7qOpqLQuXy",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV2X9aU9QxHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to check accuracy..\n",
        "def accuracy(target, predicted):\n",
        "    \n",
        "    acc = (target==predicted).sum()/len(target)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grkjR1rgQ211",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracy(predicted[\"target\"], predicted[\"pred_idx_euc\"]))    #Accuracy for euclidean Distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uopD-9KqRAkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(accuracy(predicted[\"target\"], predicted[\"pred_idx_cos\"]))    #Accuracy for Cosine Similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOgVbZvVRXwn",
        "colab_type": "text"
      },
      "source": [
        "## Combed Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv36m3J1RW7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = []\n",
        "for i in range(predicted.shape[0]):\n",
        "    if predicted.iloc[i,10] == predicted.iloc[i,11]:\n",
        "        label.append(predicted.iloc[i,10])\n",
        "    else:\n",
        "        label.append((predicted.iloc[i,10],predicted.iloc[i,10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELpyQvoPRc64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = 0\n",
        "for i in range(75206):\n",
        "    item = predicted[\"target\"][i]\n",
        "    try:\n",
        "        if label[i] == predicted[\"target\"][i]: ct +=1\n",
        "    except:\n",
        "        if item in label[i]: ct +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEQYAQ_8Rv3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct/75206   #Accuracy.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BNoPZAeSC6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}